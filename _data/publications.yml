- title: 'Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning'
  date:  'Dec 2025'
  imgurl: '/images/projects/stm.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Chao-Chung Wu
      url: 'https://scholar.google.com/citations?user=K1yD3LAAAAAJ'
    - name: Zhi Rui Tam
      url: 'https://zrt.wtf/'
    - name: Chieh-Yen Lin
      url:  'https://scholar.google.com/citations?user=-4rvt4AAAAAJ'
    - name: Yun-Nung Chen 
      url:  'https://www.csie.ntu.edu.tw/~yvchen/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Hung-Yi Lee
      url:  'https://speech.ee.ntu.edu.tw/~hylee/index.php'
  desc: 'This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.'
  abstract: 'Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2501.14315'
    - name: 'Bibtex'
      url:  '/bibtex/stm.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Implicit State Estimation via Video Replanning'
  date:  'July 2025'
  imgurl: '/images/projects/ise.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Po-Chen Ko
      url:  'https://pochen-ko.github.io/'
    - name: Jiayuan Mao
      url:  'https://jiayuanm.com/'
    - name: Yu-Hsiang Fu
    - name: Hsien-Jeng Yeh 
    - name: Chu-Rong Chen
    - name: Wei-Chiu Ma
      url:  'https://www.cs.cornell.edu/~weichiu/'
    - name: Yilun Du
      url:  'https://yilundu.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'We introduce a novel framework that integrates interaction-time data into the video planning process. Our approach updates model parameters online and filters out previously failed plans during generation. This enables implicit state estimation, allowing the system to adapt dynamically without explicitly modeling unknown state variables. We evaluate our framework through extensive experiments on a new simulated manipulation benchmark, demonstrating its ability to improve replanning performance and advance the field of video-based decision-making.'
  abstract: 'Video-based representations have gained prominence in planning and decision-making due to their ability to encode rich spatiotemporal dynamics and geometric relationships. These representations enable flexible and generalizable solutions for complex tasks such as object manipulation and navigation. However, existing video planning frameworks often struggle to adapt to failures at interaction time due to the incapability to reason about uncertainties in partially observed environments. To overcome these limitations, we introduce a novel framework that integrates interaction-time data into the planning process. Our approach updates model parameters online and filters out previously failed plans during generation. This enables implicit state estimation, allowing the system to adapt dynamically without explicitly modeling unknown state variables. We evaluate our framework through extensive experiments on a new simulated manipulation benchmark, demonstrating its ability to improve replanning performance and advance the field of video-based decision-making.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/forum?id=g6wi263Szj'
    - name: 'Bibtex'
      url:  '/bibtex/ise.txt'
  publisher:  'Building Physically Plausible World Models workshop at International Conference on Machine Learning (ICML) 2025'
  oral: true
  oral_text: 'Best paper'
  status:   ''

- title: 'Action-Constrained Imitation Learning'
  date:  'May 2025'
  imgurl: '/images/projects/acil.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Chia-Han Yeh
      equal_contribution: true
    - name: Tse-Sheng Nan
      equal_contribution: true
      url: 'https://www.linkedin.com/in/tse-sheng-nan-438856268/'
    - name: Risto Vuorio
      url:  'https://vuoristo.github.io/'
    - name: Wei Hung 
    - name: Hung Yen Wu
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Ping-Chun Hsieh
      url:  'https://pinghsieh.github.io/'
  desc: 'We study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space. We tackle the mismatch of occupancy measure between the expert and the imitator through trajectory alignment and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via model predictive control, which aligns the surrogate trajectories with the expert trajectories based on the dynamic time warping distance.'
  abstract: 'Policy learning under action constraints plays a central role in ensuring safe behaviors in various robot control and resource allocation applications. In this paper, we study a new problem setting termed Action-Constrained Imitation Learning (ACIL), where an action-constrained imitator aims to learn from a demonstrative expert with larger action space. The fundamental challenge of ACIL lies in the unavoidable mismatch of occupancy measure between the expert and the imitator caused by the action constraints. We tackle this mismatch through trajectory alignment and propose DTWIL, which replaces the original expert demonstrations with a surrogate dataset that follows similar state trajectories while adhering to the action constraints. Specifically, we recast trajectory alignment as a planning problem and solve it via Model Predictive Control, which aligns the surrogate trajectories with the expert trajectories based on the Dynamic Time Warping (DTW) distance. Through extensive experiments, we demonstrate that learning from the dataset generated by DTWIL significantly enhances performance across multiple robot control tasks and outperforms various benchmark imitation learning algorithms in terms of sample efficiency.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/forum?id=NYi9B34E1e'
    - name: 'Bibtex'
      url:  '/bibtex/acil.txt'
  publisher:  'International Conference on Machine Learning (ICML) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'SimTube: Generating Simulated Video Comments through Multimodal AI and User Personas'
  date:  'Feb 2025'
  imgurl: '/images/projects/simtube.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Yu-Kai Hung
    - name: Yun-Chien Huang
    - name: Ting-Yu Su
    - name: Yen-Ting Lin
      url:  'https://yentingl.com/'
    - name: Lung-Pan Cheng
      url:  'https://www.lungpancheng.tw/'
    - name: Bryan Wang
      url:  'https://www.dgp.toronto.edu/~bryanw/'
      equal_advisory_contribution: true
    - name: Shao-Hua Sun
      url:  ''
      me: true
      equal_advisory_contribution: true
  desc: "We introduce SimTube, a generative AI system designed to simulate audience feedback in the form of video comments before a video’s release. SimTube features a computational pipeline that integrates multimodal data from the video—such as visuals, audio, and metadata—with user personas derived from a broad and diverse corpus of audience demographics, generating varied and contextually relevant feedback. Furthermore, the system’s UI allows creators to explore and customize the simulated comments."
  abstract: "Audience feedback is crucial for refining video content, yet it typically comes after publication, limiting creators’ ability to make timely adjustments. To bridge this gap, we introduce SimTube, a generative AI system designed to simulate audience feedback in the form of video comments before a video’s release. SimTube features a computational pipeline that integrates multimodal data from the video—such as visuals, audio, and metadata—with user personas derived from a broad and diverse corpus of audience demographics, generating varied and contextually relevant feedback. Furthermore, the system’s UI allows creators to explore and customize the simulated comments. Through a comprehensive evaluation—comprising quantitative analysis, crowd-sourced assessments, and qualitative user studies—we show that SimTube’s generated comments are not only relevant, believable, and diverse but often more detailed and informative than actual audience comments, highlighting its potential to help creators refine their content before release."
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2411.09577'
    - name: 'Project Page'
      url:  'https://tingyusu1786.github.io/SimTube-Project-Page/'
    - name: 'Bibtex'
      url:  '/bibtex/simtube.txt'
  publisher:  'International Conference on Intelligent User Interfaces (IUI) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search'
  date:  'Jan 2025'
  imgurl: '/images/projects/llmgs.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Max Liu
      equal_contribution: true
    - name: Chan-Hung Yu
      url: "https://chanhungyu.github.io/"
      equal_contribution: true
    - name: Wei-Hsu Lee
    - name: Cheng-Wei Hung
    - name: Yen-Chun Chen
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: "We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy — an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to improve the programs consistently."
  abstract: "Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy — an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to improve the programs consistently. Experimental results in the Karel domain demonstrate our LLM-GS framework's superior effectiveness and efficiency. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm. Moreover, we conduct experiments with two novel tasks, showing that LLM-GS enables users without programming skills and knowledge of the domain or DSL to describe the tasks in natural language to obtain performant programs."
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2405.16450'
    - name: 'Bibtex'
      url:  '/bibtex/llmgs.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'HERO: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning'
  date:  'Jan 2025'
  imgurl: '/images/projects/hero.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Ayano Hiranaka 
      url:  'https://misoshiruseijin.github.io/'
      equal_contribution: true
    - name: Shang-Fu Chen
      url:  'https://shangfuchen.github.io/'
      equal_contribution: true
    - name: Chieh-Hsin Lai
      url:  'https://chiehhsinjesselai.github.io/'
      equal_contribution: true
    - name: Dongjun Kim
      url:  'https://sites.google.com/view/dongjun-kim'
    - name: Naoki Murata
      url:  'https://scholar.google.com/citations?user=oyuTmwoAAAAJ&hl=ja'
    - name: Takashi Shibuya
      url:  'https://ai.sony/people/Takashi-Shibuya/'
    - name: Wei-Hsiang Liao
      url:  'https://ai.sony/people/Weihsiang-Liao/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
      equal_advisory_contribution: true
    - name: Yuki Mitsufuji
      url:  'https://www.yukimitsufuji.com/'
      equal_advisory_contribution: true
  desc: "To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent."
  abstract: "Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involves generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4x more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback."
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2410.05116'
    - name: 'Bibtex'
      url:  '/bibtex/hero.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs'
  date:  'Jan 2025'
  imgurl: '/images/projects/aram.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Wei Hung 
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Ping-Chun Hsieh
      url:  'https://pinghsieh.github.io/'
  desc: 'We propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to action-constrained reinforcement learning. To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process, which include additional self-loop state transitions and a penalty signal for the rejected actions.'
  abstract: 'Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint satisfaction but at the cost of either high computational burden incurred by the quadratic programs (QP) or increased architectural complexity due to the use of sophisticated generative models. In this paper, we propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to ACRL through two modifications: (i) To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. (ii) To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process (MDP), which include additional self-loop state transitions and a penalty signal for the rejected actions. This augmented MDP incentives the learned policy to stay close to the feasible action sets. Through extensive experiments in both robot control and resource allocation domains, we demonstrate that the proposed framework enjoys faster training progress, better constraint satisfaction, and a lower action inference time simultaneously than the state-of-the-art ACRL methods.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/forum?id=AgMpK7z4bz'
    - name: 'Bibtex'
      url:  '/bibtex/aram.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing'
  date:  'Jan 2025'
  imgurl: '/images/projects/qmp.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Grace Zhang
      url: 'https://gracehzhang.github.io/'
    - name: Ayush Jain
      url:  'https://ayushj240.github.io/'
    - name: Injune Hwang
      url:  'https://hij0527.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'We propose a multi-task reinforcement learning method, Q-switch Mixture of policies (QMP), that can share exploratory behavior, which can be helpful even when the optimal behaviors differ. Furthermore, as we learn each task, we can guide the exploration by sharing behaviors in a task and state dependent way. QMP learns to selectively share exploratory behavior between tasks by using a mixture of policies based on estimated discounted returns to gather training data. '
  abstract: "Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeling data between tasks. In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors. We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm. Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments."
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2302.00671'
    - name: 'Project Page'
      url:  'https://sites.google.com/view/qmp-mtrl'
    - name: 'Bibtex'
      url:  '/bibtex/qmp.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2025'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Hierarchical Programmatic Option Framework'
  date:  'Sep 2024'
  imgurl: '/images/projects/hipo.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Yu-An Lin
      equal_contribution: true
    - name: Chen-Tao Lee 
      equal_contribution: true
    - name: Chih-Han Yang
      equal_contribution: true
    - name: Guan-Ting Liu
      equal_contribution: true
      url:  'https://dannyliu15.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'We propose the Hierarchical Programmatic Option framework (HIPO), which aims to solve long and repetitive RL problems with human-readable programs as options (low-level policies). Specifically, we proposed a method that retrieves a set of effective, diverse, and compatible programs as options (programmatic options). Then, we learn a high-level policy to effectively reuse these programmatic options to solve reoccurring subtasks.'
  abstract: 'Deep reinforcement learning (deep RL) aims at learning policies to solve decision-making problems. However, previous approaches employed neural network policies for policy learning, making it hard to interpret the decision-making process. On the other hand, prior works (Trivedi et al., 2021; Liu et al., 2023; Carvalho et al., 2024) proposed to use human-readable programs as policies to increase the interpretability of the decision-making pipeline. However, programmatic policies generated by (Trivedi et al., 2021; Liu et al., 2023; Carvalho et al., 2024) can not effectively solve long and repetitive RL tasks and can not generalize to even longer horizon during testing. To solve these problems, we propose the Hierarchical Programmatic Option framework (HIPO), which aims to solve long and repetitive RL problems with human-readable programs as options (low-level policies). Specifically, we proposed a method that retrieves a set of effective, diverse, and compatible programs as options (programmatic options). Then, we learn a high-level policy to effectively reuse these programmatic options to solve reoccurring subtasks. Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks. Ablation studies justify the effectiveness of our proposed search algorithm for retrieving a set of programmatic options.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/forum?id=FeCWZviCeP'
    - name: 'Bibtex'
      url:  '/bibtex/hipo.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2024'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Diffusion Imitation from Observation'
  date:  'Sep 2024'
  imgurl: '/images/projects/difo.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Bo-Ruei Huang
      url: 'https://rayray2002.github.io/'
    - name: Chun-Kai Yang
    - name: Chun-Mao Lai
      url: 'https://www.mecoli.net/'
    - name: Dai-Jie Wu
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'Learning from Observation (LfO) aims to imitate experts by learning from state-only demonstrations without requiring action labels. We propose to integrate a diffusion model into the adversarial imitation learning from observation framework. Specifically, we employ a diffusion model to capture expert and agent transitions by generating the next state, given the current state. Then, we reformulate the learning objective to train the diffusion model as a binary classifier and use it to provide "realness" rewards for policy learning.'
  abstract: 'Learning from Observation (LfO) aims to imitate experts by learning from state-only demonstrations without requiring action labels. Existing adversarial imitation learning approaches learn a generator agent policy to produce state transitions that are indistinguishable to a discriminator that learns to classify agent and expert state transitions. Despite its simplicity in formulation, these methods are often sensitive to hyperparameters and brittle to train. Motivated by the recent success of diffusion models in generative modeling, we propose to integrate a diffusion model into the adversarial imitation learning from observation framework. Specifically, we employ a diffusion model to capture expert and agent transitions by generating the next state, given the current state. Then, we reformulate the learning objective to train the diffusion model as a binary classifier and use it to provide ``realness'' rewards for policy learning. Our proposed framework, Diffusion Imitation from Observation (DIFO), demonstrates superior performance in various continuous control domains, including navigation, locomotion, manipulation, and games.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2410.05429'
    - name: 'Project Page'
      url:  'https://nturobotlearninglab.github.io/DIFO/'
    - name: 'Bibtex'
      url:  '/bibtex/difo.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2024'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Diffusion-Reward Adversarial Imitation Learning'
  date:  'Sep 2024'
  imgurl: '/images/projects/drail.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Chun-Mao Lai
      equal_contribution: true
      url: 'https://www.mecoli.net/'
    - name: Hsiang-Chun Wang
      equal_contribution: true
    - name: Ping-Chun Hsieh
      url: 'https://pinghsieh.github.io/'
    - name: Yu-Chiang Frank Wang
      url: 'https://vllab.ee.ntu.edu.tw/ycwang.html'
    - name: Min-Hung Chen
      url: 'https://minhungchen.netlify.app/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: "This work proposes Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more precise and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator; then, we design diffusion rewards based on the classifier's output for policy learning."
  abstract: "Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, this work proposes Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more precise and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator; then, we design diffusion rewards based on the classifier's output for policy learning. We conduct extensive experiments in navigation, manipulation, and locomotion, verifying DRAIL's effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more precise and smoother rewards."
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2405.16194'
    - name: 'Project Page'
      url:  'https://nturobotlearninglab.github.io/DRAIL/'
    - name: 'Code'
      url:  'https://github.com/NVlabs/DRAIL'
    - name: 'Bibtex'
      url:  '/bibtex/drail.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2024'
  oral: false
  oral_text: ''
  status:   ''

- title: 'REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR'
  date:  'Sep 2024'
  imgurl: '/images/projects/reborn.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Liang-Hsuan Tseng 
      equal_contribution: true
    - name: En-Pei Hu
      equal_contribution: true
      url: 'https://guapaqaq.github.io/'
    - name: Cheng-Han Chiang
    - name: Yuan Tseng
    - name: Hung-Yi Lee
      url:  'https://speech.ee.ntu.edu.tw/~hylee/index.php'
    - name: Lin-shan Lee
      url:  'https://speech.ee.ntu.edu.tw/previous_version/lslNew.htm'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'We propose REBORN, which alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity.'
  abstract: 'Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model to favor segmentations that yield phoneme sequence predictions with a lower perplexity. We conduct extensive experiments and find that under the same setting, REBORN outperforms all prior unsupervised ASR models on LibriSpeech, TIMIT, and five non-English languages in Multilingual LibriSpeech. We comprehensively analyze why the boundaries learned by REBORN improve the unsupervised ASR performance.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2402.03988'
    - name: 'Bibtex'
      url:  '/bibtex/reborn.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2024'
  oral: false
  oral_text: ''
  status:   ''
    
- title: 'LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play'
  date:  'July 2023'
  imgurl: '/images/projects/llm_discussion.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Li-Chun Lu
      equal_contribution: true
    - name: Shou-Jen Chen
      equal_contribution: true
    - name: Tsung-Min Pai
    - name: Chan-Hung Yu
      url: "https://chanhungyu.github.io/"
    - name: Hung-Yi Lee
      url:  'https://speech.ee.ntu.edu.tw/~hylee/index.php'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs.'
  abstract: 'Large language models (LLMs) have shown exceptional proficiency in natural language processing but often fall short of generating creative and original responses to open-ended questions. To enhance LLM creativity, our key insight is to emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives. To this end, we propose LLM Discussion, a three-phase discussion framework that facilitates vigorous and diverging idea exchanges and ensures convergence to creative answers. Moreover, we adopt a role-playing technique by assigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate the efficacy of the proposed framework with the Alternative Uses Test, Similarities Test, Instances Test, and Scientific Creativity Test through both LLM evaluation and human study. Our proposed framework outperforms single-LLM approaches and existing multi-LLM frameworks across various creativity metrics.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2405.06373'
    - name: 'Bibtex'
      url:  '/bibtex/llm_discussion.txt'
  publisher:  'Conference on Language Modeling (COLM) 2024'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Diffusion Model-Augmented Behavioral Cloning'
  date:  'Feb 2023'
  imgurl: '/images/projects/dbc.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shang-Fu Chen
      equal_contribution: true
    - name: Hsiang-Chun Wang
      equal_contribution: true
    - name: Ming-Hao Hsu
      url:  'https://qaz159qaz159.github.io/'
    - name: Chun-Mao Lai
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'This work aims to augment BC by employing diffusion models for modeling expert behaviors, and designing a learning objective that leverages learned diffusion models to guide policy learning. To this end, we propose diffusion model-augmented behavioral cloning (Diffusion-BC) that combines our proposed diffusion model guided learning objective with the BC objective, which complements each other. Our proposed method outperforms baselines or achieves competitive performance in various continuous control domains, including navigation, robot arm manipulation, and locomotion.'
  abstract: 'Imitation learning addresses the challenge of learning by observing an expert demonstrations without access to reward signals from the environment. Behavioral cloning (BC) formulates imitation learning as a supervised learning problem and learns from sampled state-action pairs. Despite its simplicity, it often fails to capture the temporal structure of the task and the global information of expert demonstrations. This work aims to augment BC by employing diffusion models for modeling expert behaviors, and designing a learning objective that leverages learned diffusion models to guide policy learning. To this end, we propose diffusion model-augmented behavioral cloning (Diffusion-BC) that combines our proposed diffusion model guided learning objective with the BC objective, which complements each other. Our proposed method outperforms baselines or achieves competitive performance in various continuous control domains, including navigation, robot arm manipulation, and locomotion. Ablation studies justify our design choices and investigate the effect of balancing the BC and our proposed diffusion model objective.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2302.13335'
    - name: 'Project Page'
      url:  'https://nturobotlearninglab.github.io/dbc/'
    - name: 'Code'
      url:  'https://github.com/NTURobotLearningLab/dbc'
    - name: 'Bibtex'
      url:  '/bibtex/dbc.txt'
  publisher:  'International Conference on Machine Learning (ICML) 2024'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Learning to Act from Actionless Videos through Dense Correspondences'
  date:  'Nov 2023'
  imgurl: '/images/projects/avdc.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Po-Chen Ko
      url:  'https://pochen-ko.github.io/'
    - name: Jiayuan Mao
      url:  'https://jiayuanm.com/'
    - name: Yilun Du
      url:  'https://yilundu.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Joshua B. Tenenbaum
      url:  'https://cocosci.mit.edu/josh'
  desc: 'Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that “hallucinate” robots executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute in an environment without requiring any explicit action labels, allowing us to learn from RGB videos and acquire various robotic tasks.'
  abstract: 'In this work, we present an approach to construct a video-based robot policy capable of reliably executing diverse tasks across different robots and environments from few video demonstrations without using any action annotations. Our method leverages images as a task-agnostic representation, encoding both the state and action information, and text as a general representation for specifying robot goals. By synthesizing videos that “hallucinate” robot executing actions and in combination with dense correspondences between frames, our approach can infer the closed-formed action to execute to an environment without the need of any explicit action labels. This unique capability allows us to train the policy solely based on RGB videos and deploy learned policies to various robotic tasks. We demonstrate the efficacy of our approach in learning policies on table-top manipulation and navigation tasks. Additionally, we contribute an open-source framework for efficient video modeling, enabling the training of high-fidelity policy models with four GPUs within a single day.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2310.08576'
    - name: 'Project Page'
      url:  'https://flow-diffusion.github.io/'
    - name: 'Bibtex'
      url:  '/bibtex/avdc.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2024'
  oral: true
  oral_text: 'Spotlight'
  status:   ''

- title: 'Integrating Planning and Deep Reinforcement Learning via Automatic Induction of Task Substructures'
  date:  'Nov 2023'
  imgurl: '/images/projects/planning_drl.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Jung-Chun Liu
      url:  'https://yore0403.github.io/academic/index.html'
    - name: Chi-Hsien Chang
      url:  'https://chi-hsienchang.github.io/salima/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Tian-Li Yu
      url:  'https://www.ee.ntu.edu.tw/profile1.php?id=745'
  desc: 'We propose a framework that integrates deep reinforcement learning with classical planning by automatically inducing task structures and substructures from a few demonstrations. Specifically, we adopt abstraction mapping formulation and define critical actions that lead to the transition at the abstraction level. Then, we propose to induce critical action schemata regarded as subtasks by employing genetic programming where the program model reflects prior domain knowledge of effect rules.'
  abstract: 'Despite recent advancements, deep reinforcement learning (DRL) still struggles at learning sparse-reward goal-directed tasks, while classical planning excels at addressing hierarchical tasks, yet most of the methods rely on assumptions about pre-defined subtasks. To bridge the best of both worlds, we propose a framework that integrates DRL with classical planning by automatically inducing task structures and substructures from a few demonstrations. Specifically, we adopt abstraction mapping formulation and define critical actions that lead to the transition at the abstraction level. The framework induces critical action schemata regarded as subtasks to solve the problems. Symbolic regression is used for substructure induction by employing genetic programming where the program model reflects prior domain knowledge of effect rules. We compare the proposed framework to state-of-the-art DRL algorithms, imitation learning methods, and an exploration approach in various domains. Experimental results on various tasks show that our proposed framework outperforms all the abovementioned algorithms in terms of sample efficiency and task performance. Moreover, our framework achieves strong generalization performance by effectively inducing new rules and composing task structures. Ablation studies justify the design of our induction module and the proposed genetic programming procedure.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/forum?id=PR6RMsxuW7'
    - name: 'Bibtex'
      url:  '/bibtex/planning_drl.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2024'
  oral: false
  oral_text: ''
  status:   ''  

- title: 'Addressing Long-Horizon Tasks by Integrating Program Synthesis and State Machines'
  date:  'Nov 2023'
  imgurl: '/images/projects/pomp.png'
  imgprop: 'frame'
  selected: false
  authors:
    - name: Yu-An Lin
      equal_contribution: true
    - name: Chen-Tao Lee 
      equal_contribution: true
    - name: Guan-Ting Liu
      equal_contribution: true
      url:  'https://dannyliu15.github.io/'
    - name: Pu-Jen Cheng
      url:  'https://www.csie.ntu.edu.tw/~pjcheng/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'This work proposes Program Machine Policies (POMPs), which bridge the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, compatible programs. Then, we use these programs as modes of a state machine and learn a transition function to transition among mode programs, allowing for capturing long-horizon repetitive behaviors.'
  abstract: 'Deep reinforcement learning excels in various domains but lacks generalizability and interoperability. Programmatic RL methods (Trivedi et al., 2021; Liu et al., 2023) reformulate solving RL tasks as synthesizing interpretable programs that can be executed in the environments. Despite encouraging results, these methods are limited to short-horizon tasks. On the other hand, representing RL policies using state machines (Inala et al., 2020) can inductively generalize to long-horizon tasks; however, it struggles to scale up to acquire diverse and complex behaviors. This work proposes Program Machine Policies (POMPs), which bridge the advantages of programmatic RL and state machine policies, allowing for the representation of complex behaviors and the address of long-term tasks. Specifically, we introduce a method that can retrieve a set of effective, diverse, compatible programs. Then, we use these programs as modes of a state machine and learn a transition function to transition among mode programs, allowing for capturing long-horizon repetitive behaviors. Our proposed framework outperforms programmatic RL and deep RL baselines on various tasks and demonstrates the ability to generalize to even longer horizons without any fine-tuning inductively. Ablation studies justify the effectiveness of our proposed search algorithm for retrieving a set of programs as modes.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2311.15960'
    - name: 'Slide'
      url:  './slides/pomp.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/pomp.txt'
  publisher:  'Generalization in Planning Workshop at Neural Information Processing Systems (NeurIPS) 2023'
  oral: true
  oral_text: 'Contributed talk'
  status:   ''  

- title: 'Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance'
  date:  'Nov 2023'
  imgurl: '/images/projects/boss.gif'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Jesse Zhang
      url:  'https://jesbu1.github.io/'
    - name: Jiahui Zhang 
      url:  'https://jiahui-3205.github.io/'
    - name: Karl Pertsch
      url:  'https://kpertsch.github.io/'
    - name: Ziyi Liu
      url:  'https://www.linkedin.com/in/ziyi-liu-318616186/'
    - name: Xiang Ren
      url:  'https://shanzhenren.github.io/'
    - name: Minsuk Chang
      url:  'https://minsukchang.com/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Joseph J. Lim
      url:  'https://clvrai.com/web_lim/'
  desc: 'Our approach BOSS (<strong>BO</strong>ot<strong>S</strong>trapping your own <strong>S</strong>kills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. '
  abstract: 'We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping procedure outperform those trained with naive bootstrapping as well as prior unsupervised skill acquisition methods on zero-shot execution of unseen, long-horizon tasks in new environments.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/pdf?id=a0mFRgadGO'
    - name: 'Project Page'
      url:  'https://clvrai.github.io/boss/'
    - name: 'Bibtex'
      url:  '/bibtex/boss.txt'
  publisher:  'Conference on Robot Learning (CoRL) 2023'
  oral: true
  oral_text: 'Oral'
  status:   ''

- title: 'Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs'
  date:  'Jan 2023'
  imgurl: '/images/projects/hprl.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Guan-Ting Liu
      equal_contribution: true
      url:  'https://dannyliu15.github.io/'
    - name: En-Pei Hu
      equal_contribution: true
      url:  'https://guapaqaq.github.io/'
    - name: Pu-Jen Cheng
      url:  'https://www.csie.ntu.edu.tw/~pjcheng/'
    - name: Hung-Yi Lee
      url:  'https://speech.ee.ntu.edu.tw/~hylee/index.php'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'We re-formulate solving a reinforcement learning task as synthesizing a task-solving program that can be executed to interact with the environment and maximize the return. We first learn a program embedding space that continuously parameterizes a diverse set of programs sampled from a program dataset. Then, we train a meta-policy, whose action space is the learned program embedding space, to produce a series of programs (i.e., predict a series of actions) to yield a composed task-solving program.'
  abstract: 'Aiming to produce reinforcement learning (RL) policies that are human-interpretable and can generalize better to novel scenarios, Trivedi et al. (2021) present a method (LEAPS) that first learns a program embedding space to continuously parameterize diverse programs from a pre-generated program dataset, and then searches for a task-solving program in the learned program embedding space when given a task. Despite encouraging results, the program policies that LEAPS can produce are limited by the distribution of the program dataset. Furthermore, during searching, LEAPS evaluates each candidate program solely based on its return, failing to precisely reward correct parts of programs and penalize incorrect parts. To address these issues, we propose to learn a meta-policy that composes a series of programs sampled from the learned program embedding space. By composing programs, our proposed method can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors. We design and conduct extensive experiments in the Karel domain. The experimental results show that our proposed framework outperforms baselines. The ablation studies confirm the limitations of LEAPS and justify our design choices.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2301.12950'
    - name: 'Project Page'
      url:  'https://nturobotlearninglab.github.io/hprl/'
    - name: 'Slide'
      url:  'https://nturobotlearninglab.github.io/hprl/asset/hprl_slide.pdf'
    - name: 'Poster'
      url:  'https://nturobotlearninglab.github.io/hprl/asset/hprl_poster.png'
    - name: 'Bibtex'
      url:  '/bibtex/hprl.txt'
  publisher:  'International Conference on Machine Learning (ICML) 2023'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Location-Aware Visual Question Generation with Lightweight Models'
  date:  'Nov 2023'
  imgurl: '/images/projects/locavqg.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Nicholas Collin Suwono
    - name: Justin Chih-Yao Chen
      url:  'https://dinobby.github.io/'
    - name: Tun Min Hung
    - name: Ting-Hao Kenneth Huang
      url:  'https://crowd.ist.psu.edu/'
    - name: I-Bin Liao
    - name: Yung-Hui Li
    - name: Lun-Wei Ku
      url:  'https://academiasinicanlplab.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location (e.g.,surrounding images and its GPS coordinate). To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. We propose methods to train lightweight models which can reliably generate engaging questions from location-aware information. '
  abstract: 'This work introduces a novel task, location-aware visual question generation (LocaVQG), which aims to generate engaging questions from data relevant to a particular geographical location. Specifically, we represent such location-aware information with surrounding images and a GPS coordinate. To tackle this task, we present a dataset generation pipeline that leverages GPT-4 to produce diverse and sophisticated questions. Then, we aim to learn a lightweight model that can address the LocaVQG task and fit on an edge device, such as a mobile phone. To this end, we propose a method which can reliably generate engaging questions from location-aware information. Our proposed method outperforms baselines regarding human evaluation (e.g., engagement, grounding, coherence) and automatic evaluation metrics (e.g., BERTScore, ROUGE-2). Moreover, we conduct extensive ablation studies to justify our proposed techniques for both generating the dataset and solving the task.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2310.15129'
    - name: 'Code'
      url:  'https://github.com/AcademiaSinicaNLPLab/LocaVQG'
    - name: 'Bibtex'
      url:  '/bibtex/locavqg.txt'
  publisher:  'Empirical Methods in Natural Language Processing (EMNLP) 2023'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Hierarchical Neural Program Synthesis'
  date:  'Mar 2023'
  imgurl: '/images/projects/hnps.png'
  imgprop: 'frame'
  selected: false
  authors:
    - name: Linghan Zhong
      url:  'https://thoughtp0lice.github.io/'
    - name: Ryan Lindeborg
      url:  'https://www.ryanlindeborg.com/'
    - name: Jesse Zhang
      url:  'https://jesbu1.github.io/'
    - name: Joseph J. Lim
      url:  'https://clvrai.com/web_lim/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
  desc: 'Recent works in program synthesis have demonstrated encouraging results in a variety of domains such as string transformation, tensor manipulation, and describing behaviors of embodied agents. Most existing program synthesis methods are designed to synthesize programs from scratch, generating a program token by token, line by line. This fundamentally prevents these methods from scaling up to synthesize programs that are longer or more complex. In this work, we present a scalable program synthesis framework that instead synthesizes a program by hierarchically composing programs.'
  abstract: 'Recent works in program synthesis have demonstrated encouraging results in a variety of domains such as string transformation, tensor manipulation, and describing behaviors of embodied agents. Most existing program synthesis methods are designed to synthesize programs from scratch, generating a program token by token, line by line. This fundamentally prevents these methods from scaling up to synthesize programs that are longer or more complex. In this work, we present a scalable program synthesis framework that instead synthesizes a program by hierarchically composing programs. The experimental results demonstrate that the proposed framework can synthesize programs that are significantly longer and more complex than the programs considered in prior program synthesis works.'
  tags:
    - name: 'Paper'
      url:  'http://arxiv.org/abs/2303.06018'
    - name: 'Bibtex'
      url:  '/bibtex/hnps.txt'
    - name: 'Project Page'
      url:  'https://thoughtp0lice.github.io/hnps_web/'
  publisher:  'arXiv preprint'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Skill-based Meta-Reinforcement Learning'
  date:  'Dec 2021'
  imgurl: '/images/projects/simpl.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Taewook Nam
      url: 'https://www.mlai-kaist.com/people'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Karl Pertsch
      url:  'https://kpertsch.github.io/'
    - name: Sung Ju Hwang
      url:  'http://www.sungjuhwang.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'We devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task.'
  abstract: 'While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form of offline datasets without reward or task annotations. While these approaches yield improved sample efficiency, millions of interactions with environments are still required to solve complex tasks. In this work, we devise a method that enables meta-learning on long-horizon, sparse-reward tasks, allowing us to solve unseen target tasks with orders of magnitude fewer environment interactions. Our core idea is to leverage prior experience extracted from offline datasets during meta-learning. Specifically, we propose to (1) extract reusable skills and a skill prior from offline datasets, (2) meta-train a high-level policy that learns to efficiently compose learned skills into long-horizon behaviors, and (3) rapidly adapt the meta-trained policy to solve an unseen target task. Experimental results on continuous control tasks in navigation and manipulation demonstrate that the proposed method can efficiently solve long-horizon novel target tasks by combining the strengths of meta-learning and the usage of offline datasets, while prior approaches in RL, meta-RL, and multi-task RL require substantially more environment interactions to solve the tasks.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/pdf?id=jeLW-Fh9bV'
    - name: 'Project Page'
      url:  'https://namsan96.github.io/SiMPL/'
    - name: 'Code'
      url:  'https://github.com/namsan96/simpl'
    - name: 'Bibtex'
      url:  '/bibtex/simpl.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2022'
  oral: false
  oral_text: ''
  status:   ''
- title: 'Learning to Synthesize Programs as Interpretable and Generalizable Policies'
  date:  'Aug 2021'
  imgurl: '/images/projects/leaps.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Dweep Trivedi
      equal_contribution: true
    - name: Jesse Zhang
      url:  'https://jesbu1.github.io/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'We present a framework that learns to synthesize a program, detailing the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task.'
  abstract: 'Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network poli- cies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving pro- grams but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/2108.13643'
    - name: 'Project Page'
      url:  'https://clvrai.github.io/leaps/'
    - name: 'Code'
      url:  'https://github.com/clvrai/leaps'
    - name: 'Slide'
      url:  './images/slides/leaps.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/leaps.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2021'
  oral: false
  oral_text: ''
  status:   ''
    
- title: 'Generalizable Imitation Learning from Observation via Inferring Goal Proximity'
  date:  'Aug 2021'
  imgurl: '/images/projects/goal.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Youngwoon Lee
      equal_contribution: true
      url:  'http://youngwoon.github.io/'
    - name: Andrew Szot
      equal_contribution: true
      url:  'https://www.andrewszot.com/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator, for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training.' 
  abstract: 'Task progress is intuitive and readily available task information that can guide an agent closer to the desired goal. Furthermore, a progress estimator can generalize to new situations. From this intuition, we propose a simple yet effective imitation learning from observation method for a goal-directed task using a learned goal proximity function as a task progress estimator, for better generalization to unseen states and goals. We obtain this goal proximity function from expert demonstrations and online agent experience, and then use the learned goal proximity as a dense reward for policy training. We demonstrate that our proposed method can robustly generalize compared to prior imitation learning methods on a set of goal-directed tasks in navigation, locomotion, and robotic manipulation, even with demonstrations that cover only a part of the states.'
  tags:
    - name: 'Paper'
      url: 'https://papers.nips.cc/paper/2021/hash/868b7df964b1af24c8c0a9e43a330c6a-Abstract.html'
    - name: 'Project Page'
      url:  'https://clvrai.github.io/goal_prox_il/'
    - name: 'Code'
      url:  'https://github.com/clvrai/goal_prox_il'
    - name: 'Slide'
      url:  '/images/slides/goal_proximity.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/goal.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2021'
  oral: false
  oral_text: ''
  status:   ''

- title: 'Program Guided Agent'
  date:  'Apr 2020'
  imgurl: '/images/projects/pga.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Te-Lin Wu
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'We propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks, instead of natural languages which can often be ambiguous. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task.'
  abstract: 'Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/pdf?id=BkxUvnEYDH'
    - name: 'Project Page'
      url:  'https://shaohua0116.github.io/ProgramGuidedAgent/'
    - name: 'Slide'
      url:  './images/slides/pga.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/pga.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2020'
  oral: true 
  oral_text: 'Spotlight'
  status:   ''
 
- title: 'Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation'
  date:  'Dec 2019'
  imgurl: '/images/projects/mmaml.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Risto Vuorio
      url:  'https://vuoristo.github.io/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  ''
      me: true
      equal_contribution: true
    - name: Hexiang Hu
      url:  'http://hexianghu.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Model-agnostic meta-learners aim to acquire meta-prior parameters from a distribution of tasks and adapt to novel tasks with few gradient updates. Yet, seeking a common initialization shared across the entire task distribution substantially limits the diversity of the task distributions that they are able to learn from. We propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior according to the identified mode, allowing more efficient fast adaptation.'
  abstract: 'Model-agnostic meta-learners aim to acquire meta-prior parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that learning from a multimodal distribution could benefit learning the tasks from a single mode.'
  tags:
    - name: 'Paper'
      url:  'http://papers.nips.cc/paper/8296-multimodal-model-agnostic-meta-learning-via-task-aware-modulation'
    - name: 'Project Page'
      url:  'https://vuoristo.github.io/MMAML/'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/MMAML-Classification'
    - name: 'Poster'
      url:  './images/posters/mmaml.pdf'
    - name: 'Spotlight Slide'
      url:  './images/slides/mmaml.pdf'
    - name: 'Spotlight Talk'
      url:  'https://slideslive.com/38921753/track-3-session-4'
    - name: 'Bibtex'
      url:  '/bibtex/mmaml.txt'
  publisher:  'Neural Information Processing Systems (NeurIPS) 2019'
  oral: true 
  oral_text: 'Spotlight'
  status:   ''
  # place:    'in Vancouver, Canada'
 
- title: 'Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks'
  date:  'June 2019'
  imgurl: '/images/projects/fal.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Minyoung Huh
      url:  'http://minyounghuh.com/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  ''
      equal_contribution: true
      me: true
    - name: Ning Zhang
      url:  'https://people.eecs.berkeley.edu/~nzhang/'
  desc: 'We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the generator conditions on the discriminator spatial output response and its previous generation to improve generation quality over time - allowing the generator to attend and fix its previous mistakes. <!--more--> To effectively utilize the feedback, we propose an adaptive spatial transform (AST) layer, which learns to spatially modulate feature maps from its previous generation and the feedback signal from the discriminator.'
  abstract: 'We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the discriminator feedback is integrated into the feedforward path of the generation process. Specifically, the generator conditions on the discriminator spatial output response and its previous generation to improve generation quality over time -- allowing the generator to attend and fix its previous mistakes. %%% technical contribution To effectively utilize the feedback, we propose an adaptive spatial transform (AST) layer, which learns to spatially modulate feature maps from its previous generation and the feedback signal from the discriminator. We demonstrate that one can easily adapt our method to improve existing adversarial learning frameworks on a wide range of tasks, including image generation, image-to-image translation, and voxel generation.'
  tags:
    - name: 'Paper'
      url:  'http://openaccess.thecvf.com/content_CVPR_2019/html/Huh_Feedback_Adversarial_Learning_Spatial_Feedback_for_Improving_Generative_Adversarial_Networks_CVPR_2019_paper.html'
    - name: 'Poster'
      url:  './images/posters/fal.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/fal.txt'
  publisher:  'IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019'
  oral: false
  oral_text: ''
  status:   ''
  # place:    'in Long Beach, California, USA'
 
- title: 'Composing Complex Skills by Learning Transition Policies'
  date:  'May 2019'
  imgurl: '/images/projects/transition.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Youngwoon Lee
      url:  'http://youngwoon.github.io/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  ''
      equal_contribution: true
      me: true
    - name: Sriram Somasundaram
      url:  'http://srirams32.github.io/'
    - name: Edward Hu
      url:  'https://edwardshu.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. <!--more--> The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional methods struggle at.'
  abstract: 'Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at. We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills. The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/pdf?id=rygrBhC5tQ'
    - name: 'Project Page'
      url:  'https://youngwoon.github.io/transition/'
    - name: 'Code'
      url:  'https://github.com/youngwoon/transition'
    - name: 'Slide'
      url:  './images/slides/transition.pdf'
    - name: 'Poster'
      url:  './images/posters/transition.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/transition.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2019'
  oral: false
  oral_text: ''
  status:   ''
  # place:    'in New Orleans, USA'
    
- title: 'Toward Multimodal Model-Agnostic Meta-Learning'
  date:  'Dec 2018'
  imgurl: '/images/projects/mumomaml.png'
  imgprop: 'frame'
  selected: false
  authors:
    - name: Risto Vuorio
      url:  'https://vuoristo.github.io/'
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Hexiang Hu
      url:  'http://hexianghu.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Model-agnostic meta-learners aim to acquire meta-prior parameters from a distribution of tasks and adapt to novel tasks with few gradient updates. Yet, seeking a common initialization shared across the entire task distribution substantially limits the diversity of the task distributions that they are able to learn from. We propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior according to the identified mode, allowing more efficient fast adaptation.'
  abstract: 'Model-agnostic meta-learners aim to acquire meta-prior parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that learning from a multimodal distribution could benefit learning the tasks from a single mode.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/abs/1812.07172'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/MMAML-Classification'
    - name: 'Bibtex'
      url:  '/bibtex/mumomaml.txt'
  publisher:  'Meta-Learning Workshop at Neural Information Processing Systems (NeurIPS) 2018'
  oral: false
  oral_text: ''
  status:   ''
  # place:    'in Vancouver, Canada'
      
- title: 'Multi-view to Novel View: Synthesizing Novel Views with Self-Learned Confidence'
  date:  'Sep. 2018'
  imgurl: '/images/projects/view.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Minyoung Huh
      url:  'http://minyounghuh.com/'
    - name: Yuan-Hong Liao
      url:  'https://andrewliao11.github.io/'
    - name: Ning Zhang
      url:  'https://people.eecs.berkeley.edu/~nzhang/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  publisher:  'European Conference on Computer Vision (ECCV) 2018'
  oral: false
  oral_text: ''
  status:   ''
  # place:    'in Munich, Germany'
  desc: 'We aim to synthesize a target image with an arbitrary camera pose from multipple given source images. We propose an end-to-end trainable framework which consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. We introduce a self-learned confidence aggregation mechanism to merge the predictions produced by the two modules given multi-view source images.'
  abstract: 'We address the task of multi-view novel view synthesis, where we are interested in synthesizing a target image with an arbitrary camera pose from given source images. We propose an end-to-end trainable framework that learns to exploit multiple viewpoints to synthesize a novel view without any 3D supervision. Specifically, our model consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. To merge the predictions produced by the two modules given multi-view source images, we introduce a self-learned confidence aggregation mechanism. We evaluate our model on images rendered from 3D object models as well as real and synthesized scenes. We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.'
  tags:
    - name: 'Paper'
      url:  'https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf'
    - name: 'Project Page'
      url:  'https://shaohua0116.github.io/Multiview2Novelview'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/Multiview2Novelview'
    - name: 'Poster'
      url:  './images/posters/view.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/multiview.txt'

- title: 'Neural Program Synthesis from Diverse Demonstration Videos'
  date:  'Jul. 2018'
  imgurl: '/images/projects/demo2program.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shao-Hua Sun
      url:  ''
      equal_contribution: true
      me: true
    - name: Hyeonwoo Noh
      url:  'http://cvlab.postech.ac.kr/~hyeonwoonoh/'
      equal_contribution: true
    - name: Sriram Somasundaram
      url:  'http://srirams32.github.io/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  publisher:  'International Conference on Machine Learning (ICML) 2018'
  status:   ''
  # place:    'in Stockholm, Sweden'
  desc: 'Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a framework that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module to improve the network’s ability to integrate multiple demonstrations and employ a multi-task objective to encourage the model to learn meaningful intermediate representations.'
  abstract: 'Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a multi-task objective to encourage the model to learn meaningful intermediate representations for end-to-end training. We show that our model is able to reliably synthesize underlying programs as well as capture diverse behaviors exhibited in demonstrations.'
  tags:
    - name: 'Paper'
      url:  'http://proceedings.mlr.press/v80/sun18a/sun18a.pdf'
    - name: 'Project Page'
      url:  'https://shaohua0116.github.io/demo2program/'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/demo2program'
    - name: 'Slide'
      url:  './images/slides/demo2program.pdf'
    - name: 'Poster'
      url:  './images/posters/demo2program.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/demo2program.txt'
  oral: false
  oral_text: 'Short talk'

- title: 'Exploiting Image Structural Similarity for Single Image Rain Removal'
  date:  'Oct. 2014'
  imgurl: '/images/projects/rain.png'
  imgprop: 'frame'
  selected: false
  authors:
    - name: Shao-Hua Sun
      url:  ''
      me: true
    - name: Shang-Pu Fan
      url:  '#'
    - name: Yu-Chiang Frank Wang
      url:  'http://vllab.ee.ntu.edu.tw/members.html'
  publisher:  'International Conference on Image Processing (ICIP) 2014'
  oral: false
  status:   ''
  # place:    'in Paris, France'
  desc: 'Without any prior knowledge or user interaction, single image rain removal has been a challenging task. By observing the limitations of standard batch-mode learning-based methods, we propose to exploit the structural similarity of the image bases for solving this task. By formulating the basis selection as an optimization problem, we are able to disregard those associated with rain patterns while the detailed image information can be preserved. Experiments on both synthetic and real-world images will verify the effectiveness of our proposed method.'
  tags:
    - name: 'Paper'
      url:  'http://mml.citi.sinica.edu.tw/papers/ICIP_2014_Sun.pdf'
    - name: 'Bibtex'
      url:  '/bibtex/rain.txt'
