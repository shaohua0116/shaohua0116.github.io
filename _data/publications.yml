- title: 'Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks'
  date:  'June 2019'
  imgurl: '/images/projects/fal.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Minyoung Huh
      url:  'http://minyounghuh.com/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      equal_contribution: true
      me: true
    - name: Ning Zhang
      url:  'https://people.eecs.berkeley.edu/~nzhang/'
  desc: 'We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the discriminator feedback is integrated into the feedforward path of the generation process. Specifically, the generator conditions on the discriminator spatial output response and its previous generation to improve generation quality over time -- allowing the generator to attend and fix its previous mistakes. To effectively utilize the feedback, we propose an adaptive spatial transform (AST) layer, which learns to spatially modulate feature maps from its previous generation and the feedback signal from the discriminator.'
  abstract: 'We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the discriminator feedback is integrated into the feedforward path of the generation process. Specifically, the generator conditions on the discriminator spatial output response and its previous generation to improve generation quality over time -- allowing the generator to attend and fix its previous mistakes. %%% technical contribution To effectively utilize the feedback, we propose an adaptive spatial transform (AST) layer, which learns to spatially modulate feature maps from its previous generation and the feedback signal from the discriminator. We demonstrate that one can easily adapt our method to improve existing adversarial learning frameworks on a wide range of tasks, including image generation, image-to-image translation, and voxel generation.'
  tags:
    # - name: 'Paper'
    #   url:  'https://openreview.net/pdf?id=rygrBhC5tQ'
    - name: 'Bibtex'
      url:  './bibtex/fal.txt'
  publisher:  'IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2019'
  status:   ''
  # place:    'in Long Beach, California, USA'
 
- title: 'Composing Complex Skills by Learning Transition Policies'
  date:  'May 2019'
  imgurl: '/images/projects/transition.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Youngwoon Lee
      url:  'http://youngwoon.github.io/'
      equal_contribution: true
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      equal_contribution: true
      me: true
    - name: Sriram Somasundaram
      url:  'http://srirams32.github.io/'
    - name: Edward Hu
      url:  'https://edwardshu.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional methods struggle at.'
  abstract: 'Humans acquire complex skills by exploiting previously learned skills and making transitions between them. To empower machines with this ability, we propose a method that can learn transition policies which effectively connect primitive skills to perform sequential tasks without handcrafted rewards. To efficiently train our transition policies, we introduce proximity predictors which induce rewards gauging proximity to suitable initial states for the next skill. The proposed method is evaluated on a set of complex continuous control tasks in bipedal locomotion and robotic arm manipulation which traditional policy gradient methods struggle at. We demonstrate that transition policies enable us to effectively compose complex skills with existing primitive skills. The proposed induced rewards computed using the proximity predictor further improve training efficiency by providing more dense information than the sparse rewards from the environments.'
  tags:
    - name: 'Paper'
      url:  'https://openreview.net/pdf?id=rygrBhC5tQ'
    - name: 'Project Page'
      url:  'https://youngwoon.github.io/transition/'
    - name: 'OpenReview'
      url:  'https://openreview.net/forum?id=rygrBhC5tQ'
    - name: 'Bibtex'
      url:  './bibtex/transition.txt'
  publisher:  'International Conference on Learning Representations (ICLR) 2019'
  status:   ''
  # place:    'in New Orleans, USA'
    
- title: 'Toward Multimodal Model-Agnostic Meta-Learning'
  date:  'Dec. 2018'
  imgurl: '/images/projects/mumomaml.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Risto Vuorio
      url:  'https://vuoristo.github.io/'
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      me: true
    - name: Hexiang Hu
      url:  'http://hexianghu.com/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  desc: 'Gradient-based meta-learners such as MAML are able to learn a meta-prior from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. One important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify tasks sampled from a multimodal task distribution and adapt quickly through gradient updates.'
  abstract: 'Gradient-based meta-learners such as MAML are able to learn a meta-prior from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. One important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML with the capability to identify tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML algorithm that is able to modulate its meta-learned prior according to the identified task, allowing faster adaptation. We evaluate the proposed model on a diverse set of problems including regression, few-shot image classification, and reinforcement learning. The results demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks sampled from a multimodal distribution.'
  tags:
    - name: 'Paper'
      url:  'https://arxiv.org/pdf/1812.07172.pdf'
    - name: 'Project Page'
      url:  './mumomaml/index.html'
    - name: 'arXiv'
      url:  'https://arxiv.org/abs/1812.07172'
    - name: 'Poster'
      url:  './images/posters/mumomaml.pdf'
    - name: 'Bibtex'
      url:  './bibtex/mumomaml.txt'
  publisher:  'Meta-Learning Workshop at Neural Information Processing Systems (NeurIPS) 2018'
  status:   ''
  # place:    'in Montreal, Canada'
      
- title: 'Multi-view to Novel View: Synthesizing Novel Views with Self-Learned Confidence'
  date:  'Sep. 2018'
  imgurl: '/images/projects/view.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      me: true
    - name: Minyoung Huh
      url:  'http://minyounghuh.com/'
    - name: Yuan-Hong Liao
      url:  'https://andrewliao11.github.io/'
    - name: Ning Zhang
      url:  'https://people.eecs.berkeley.edu/~nzhang/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  publisher:  'European Conference on Computer Vision (ECCV) 2018'
  status:   ''
  # place:    'in Munich, Germany'
  desc: 'We address the task of multi-view novel view synthesis, where we are interested in synthesizing a target image with an arbitrary camera pose from given source images. We propose an end-to-end trainable framework which consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. We introduce a self-learned confidence aggregation mechanism to merge the predictions produced by the two modules given multi-view source images.'
  abstract: 'We address the task of multi-view novel view synthesis, where we are interested in synthesizing a target image with an arbitrary camera pose from given source images. We propose an end-to-end trainable framework that learns to exploit multiple viewpoints to synthesize a novel view without any 3D supervision. Specifically, our model consists of a flow prediction module and a pixel generation module to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors. To merge the predictions produced by the two modules given multi-view source images, we introduce a self-learned confidence aggregation mechanism. We evaluate our model on images rendered from 3D object models as well as real and synthesized scenes. We demonstrate that our model is able to achieve state-of-the-art results as well as progressively improve its predictions when more source images are available.'
  tags:
    - name: 'Paper'
      url:  'https://shaohua0116.github.io/Multiview2Novelview/sun2018multiview.pdf'
    - name: 'Project Page'
      url:  'https://shaohua0116.github.io/Multiview2Novelview'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/Multiview2Novelview'
    - name: 'Poster'
      url:  './images/posters/view.pdf'
    - name: 'Bibtex'
      url:  './bibtex/multiview.txt'

- title: 'Neural Program Synthesis from Diverse Demonstration Videos'
  date:  'Jul. 2018'
  imgurl: '/images/projects/demo2program.png'
  imgprop: 'frame'
  selected: true
  authors:
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      equal_contribution: true
      me: true
    - name: Hyeonwoo Noh
      url:  'http://cvlab.postech.ac.kr/~hyeonwoonoh/'
      equal_contribution: true
    - name: Sriram Somasundaram
      url:  'http://srirams32.github.io/'
    - name: Joseph J. Lim
      url:  'http://www-bcf.usc.edu/~limjj/'
  publisher:  'International Conference on Machine Learning (ICML) 2018'
  status:   ''
  # place:    'in Stockholm, Sweden'
  desc: 'Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a multi-task objective to encourage the model to learn meaningful intermediate representations for end-to-end training.'
  abstract: 'Interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans. To empower machines with this ability, we propose a neural program synthesizer that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos. We introduce a summarizer module as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a multi-task objective to encourage the model to learn meaningful intermediate representations for end-to-end training. We show that our model is able to reliably synthesize underlying programs as well as capture diverse behaviors exhibited in demonstrations.'
  tags:
    - name: 'Paper'
      url:  'https://shaohua0116.github.io/demo2program/sun2018neural.pdf'
    - name: 'Project Page'
      url:  'https://shaohua0116.github.io/demo2program/'
    - name: 'Code'
      url:  'https://github.com/shaohua0116/demo2program'
    - name: 'Slides'
      url:  './images/slides/demo2program.pdf'
    - name: 'Poster'
      url:  './images/posters/demo2program.pdf'
    - name: 'Bibtex'
      url:  './bibtex/demo2program.txt'

- title: 'Exploiting Image Structural Similarity for Single Image Rain Removal'
  date:  'Oct. 2014'
  imgurl: '/images/projects/rain.png'
  imgprop: 'frame'
  selected: false
  authors:
    - name: Shao-Hua Sun
      url:  'https://shaohua0116.github.io/'
      me: true
    - name: Shang-Pu Fan
      url:  '#'
    - name: Yu-Chiang Frank Wang
      url:  'http://vllab.ee.ntu.edu.tw/members.html'
  publisher:  'International Conference on Image Processing (ICIP) 2014'
  status:   ''
  # place:    'in Paris, France'
  desc: 'Without any prior knowledge or user interaction, single image rain removal has been a challenging task. By observing the limitations of standard batch-mode learning-based methods, we propose to exploit the structural similarity of the image bases for solving this task. By formulating the basis selection as an optimization problem, we are able to disregard those associated with rain patterns while the detailed image information can be preserved. Experiments on both synthetic and real-world images will verify the effectiveness of our proposed method.'
  tags:
    - name: 'Paper'
      url:  'http://mml.citi.sinica.edu.tw/papers/ICIP_2014_Sun.pdf'
    - name: 'Bibtex'
      url:  './bibtex/rain.txt'
